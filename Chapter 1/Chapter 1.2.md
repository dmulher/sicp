### 1.2.1: Linear Recursion and Iteration
Here we look at recursion, and the difference between a procedure and a process. A recursive procedure is a procedure that calls itself, while a recursive process is something that calls itself multiple times (and is generally very inefficient). An iterative process, on the other hand, is something that iterates through, scaling linearly with the task.
##### Exercise 1.9
> Each of the following two procedures defines a method for adding two positive integers in terms of the procedures `inc`, which  increments its argument by 1, and `dec`, which decrements its argument by 1.
```scheme
(define (+ a b)
  (if (= a 0) b (inc (+ (dec a) b))))
(define (+ a b)
  (if (= a 0) b (+ (dec a) (inc b))))
```
> Using the substitution model, illustrate the process generated by each procedure in evaluating `(+ 4 5)`. Are these processes iterative or recursive?

```
(+ 4 5)
(inc (+ 3 5))
(inc (inc (+ 2 5)))
(inc (inc (inc (+ 1 5))))
(inc (inc (inc (inc (+ 0 5)))))
(inc (inc (inc (inc 5))))
(inc (inc (inc 6)))
(inc (inc 7))
(inc 8)
9
```

```
(+ 4 5)
(+ 3 6)
(+ 2 7)
(+ 1 8)
(+ 0 9)
9
```

The first process is a recursive process in a recursive procedure, the second is an iterative process in a recursive procedure.
##### Exercise 1.10
> The following procedure computes a mathematical function called Ackermann's function.
``` scheme
(define (A x y)
  (cond ((= y 0) 0)
        ((= x 0) (* 2 y))
        ((= y 1) 2)
        (else (A (- x 1) (A x (- y 1))))))
```
> What are the values of the following expressions?
``` scheme
> (A 1 10) = 2 ^ 10
(A 0 (A 1 9))
1024
> (A 2 4) = 2 ^ (2 ^ (2 ^ 2))
65536
> (A 3 3) = ???
65536
```
> Consider the following procedures, where `A` is the procedure function defined above:
```scheme
(define (f n) (A 0 n))
(define (g n) (A 1 n))
(define (h n) (A 2 n))
(define (k n) (* 5 n n))
```
> Give concise mathematical definitions for the functions computed by the procedures `f`, `g`, and `h` for positive integer values of `n`. For example, `(k n)` computes $5n^{2}$.

$f(n)=2n$
$g(n)=n^{2}$
$h(n)=2^{h(n-1)}$ where $h^{0}=1$
$k(n)=5n^{2}$
### 1.2.2: Tree Recursion
We look at recursion that is exponential in growth, where one call to a program generates multiple more calls to the program.
##### Challenge
Write the number of ways to make change as a linear algorithm. Original below:
```scheme
(define (count-change amount) (cc amount 5))
(define (cc amount kinds-of-coins)
  (cond ((= amount 0) 1)
        ((or (< amount 0) (= kinds-of-coins 0)) 0)
        (else (+ (cc amount
                     (- kinds-of-coins 1))
                 (cc (- amount
                        (first-denomination
                         kinds-of-coins))
                     kinds-of-coins)))))
(define (first-denomination kinds-of-coins)
  (cond ((= kinds-of-coins 1) 1)
        ((= kinds-of-coins 2) 5)
        ((= kinds-of-coins 3) 10)
        ((= kinds-of-coins 4) 25)
        ((= kinds-of-coins 5) 50)))
```
I gave up on this one, it's tough.
##### Exercise 1.11
> A function $f$ is defined by the rule that
> $f(n) = n$ if $n < 3$
> $f(n) = f(n-1) + 2f(n-2) + 3f(n-3)$ if $n >= 3$
> Write a procedure that computes $f$ by means of a recursive process.
> Write a procedure that computes $f$ by means of an iterative process.
```scheme
(define (f n)
  (if (< n 3)
	  n
      (+ (f (- n 1)) (* 2 (f (- n 2))) (* 3 (f (- n 3))))))

(define (f_i n)
  (if (< n 3)
      n
      (f_iter n 3 2 1 0)))

(define (f_iter n i i1 i2 i3)
  (if (= n i)
      (+ i1 (* 2 i2) (* 3 i3))
      (f_iter n (+ i 1) (+ i1 (* 2 i2) (* 3 i3)) i1 i2)))
```
##### Exercise 1.12
> The following pattern of numbers is called Pascal's triangle. The numbers at the edge are all 1, and each number inside the triangle is the sum of the two numbers above it. Write a procedure that computes elements of Pascal's triangle by means of a recursive process.
```scheme
(define (triangle row index)
  (cond ((> index row) 0)
        ((or (= index 1) (= index row)) 1)
        (else (+ (triangle (- row 1) (- index 1))
                 (triangle (- row 1) index)))))
```
##### Exercise 1.13
> Prove that Fib(n) is the closest integer to $\frac{φ^n}{\sqrt{5}}$, where $φ=\frac{(1+\sqrt{5})}{2}$. Hint: Let $ψ=\frac{(1-\sqrt{5})}{2}$. Use induction and the definition of the Fibonacci numbers (see Section 1.2.2) to prove that $Fib(n)=\frac{(φ^n-ψ^n)}{\sqrt{5}}$.

$Fib(n)\approx\frac{φ^n}{\sqrt{5}}$
$Fib(n)=\frac{(φ^n-ψ^n)}{\sqrt{5}}$
$Fib(n)=Fib(n-1)+Fib(n-2)$
$\frac{(φ^n-ψ^n)}{\sqrt{5}}=\frac{(φ^{n-1}-ψ^{n-1})}{\sqrt{5}}+\frac{(φ^{n-2}-ψ^{n-2})}{\sqrt{5}}$
$(φ^n-ψ^n)=(φ^{n-1}-ψ^{n-1})+(φ^{n-2}-ψ^{n-2})$
$(φ^n-ψ^n)=(\frac{φ^n}{φ}-\frac{ψ^n}{ψ})+(\frac{φ^n}{φ^2}-\frac{ψ^n}{ψ^2})$
$(φ^n-ψ^n)=φ^n(\frac{1}{φ}+\frac{1}{φ^2})-ψ^n(\frac{1}{ψ}+\frac{1}{ψ^2})$
$(φ^n-ψ^n)=φ^n(\frac{2}{(1+\sqrt{5})}+\frac{4}{(1+\sqrt{5})^2})-ψ^n(\frac{2}{(1-\sqrt{5})}+\frac{4}{(1-\sqrt{5})^2})$
$(φ^n-ψ^n)=φ^n(\frac{2(1+\sqrt{5})+4}{(1+\sqrt{5})^2})-ψ^n(\frac{2(1-\sqrt{5})+4}{(1-\sqrt{5})^2})$
$(φ^n-ψ^n)=φ^n(\frac{2(1+\sqrt{5})+4}{(1+\sqrt{5})^2})-ψ^n(\frac{2(1-\sqrt{5})+4}{(1-\sqrt{5})^2})$
$(φ^n-ψ^n)=φ^n(\frac{6+2\sqrt{5}}{(1+\sqrt{5})^2})-ψ^n(\frac{6-2\sqrt{5}}{(1-\sqrt{5})^2})$
$(φ^n-ψ^n)=φ^n(\frac{6+2\sqrt{5}}{6+2\sqrt{5}})-ψ^n(\frac{6-2\sqrt{5}}{6-2\sqrt{5}})$
$(φ^n-ψ^n)=(φ^n-ψ^n)$
### 1.2.3: Orders of Growth
Here we start to look at Big Θ notation, discussed as "Orders of Growth".
It is not overly exciting as a chapter, as this is programming 101 these days.
##### Exercise 1.14
> Draw the tree illustrating the process generated by the `count-change` procedure of Section 1.2.2 in making change for 11 cents. What are the orders of growth of the space and number of steps used by this process as the amount to be changed increases?

![[sicp_1_14.png]]
Using red as 0 terminators and green as 1 terminators.

Seems to be $Θ(n^2)$ for number of steps, as we are doing tree recursion, though an argument could be made for $Θ(n*log(n))$ or maybe even $Θ(n)$, due to how quickly it terminates on the right until it hits single digits, but I don't buy it. People seem to claim it is $Θ(n^5)$, which seems a tad excessive to me, but I guess I can sorta see it. As for space complexity, I am going to hazard $Θ(n)$.
##### Exercise 1.15
> The sine of an angle (specified in radians) can be computed by making use of the approximation $sin(x) \approx x$ if $x$ is sufficiently small, and the trigonometric identity $sin(x)=3sin(\frac{x}{3})-4sin^3(\frac{x}{3})$ to reduce the size of the argument of sin\[e\]. (For purposes of this exercise an angle is considered "sufficiently small" if its magnitude is not greater than 0.1 radians.) These ideas are incorporated in the following procedures:
```scheme
(define (cube x) (* x x x))
(define (p x) (- (* 3 x) (* 4 (cube x))))
(define (sine angle)
  (if (not (> (abs angle) 0.1))
      angle
      (p (sine (/ angle 3.0)))))
```
> a. How many times is the procedure `p` applied when `(sine 12.15)` is evaluated?

```scheme
(sine 12.15)
(p (sine 4.05))
(p (p (sine 1.35)))
(p (p (p (sine 0.45))))
(p (p (p (p (sine 0.15)))))
(p (p (p (p (p (sine 0.05))))))
(p (p (p (p (p 0.05)))))
```
5 times.

> b. What is the order of growth in space and number of steps (as a function of `a` used by the process generated by the `sine` procedure when `(sine a)` is evaluated?

$Θ(log(a))$ for number of times called, $Θ(1)$ for space consumed.
### 1.2.4: Exponentiation
Here we consider the job of exponentiation. It is really just a set up for the exercises.
##### Exercise 1.16
> Design a procedure that evolves an iterative exponentiation process that uses successive squaring and uses a logarithmic number of steps, as does `fast-expt`. (Hint: Using the oversrvation that $(b^{n/2})^2=(b^2)^{n/2}$, keep, along with the exponent `n` and the base `b`, an additional state variable `a`, and define the state transformation in such a way that the product $ab^n$ is unchanged from state to state. At the beginning of the process `a` is taken to be 1, and the answer is given by the value of `a` at the end of the process. In general, the technique of defining an *invariant quantity* that remains unchanged from state to state is a powerful way to think about the design of iterative algorithms.)

```scheme
(define (faster_exp b n) (exp_iter 1 b n))

(define (exp_iter a b n)
  (cond ((= n 0) a)
        ((even? n) (exp_iter a (* b b) (/ n 2)))
        (else (exp_iter (* a b) b (- n 1)))))
```
### Exercise 1.17
> The exponentiation algorithms in this section are based on performing exponentiation by means of repeated multiplication. In a similar way, one can perform integer multiplication by means of repeated addition. The following multiplication procedure (in which it is assumed that our language can only add, not multiply) is analogous to the `expt` procedure:
```scheme
(define (* a b)
  (if (= b 0)
      0
      (+ a (* a (- b 1)))))
```
> This algorithm takes a number of steps that is linear in `b`. Now suppose we include, together with addition, operations `double`, which doubles an integer, and `halve`, which divides an (even) integer by 2. Using these, design a multiplication procedure analogous to `fast-expt` that uses a logarithmic number of steps.

```scheme
(define (double a) (* a 2))

(define (half a) (/ a 2))

(define (fast_mult a b)
  (cond ((or (= b 0) (= a 0)) 0)
        ((= b 1) a)
        ((= a 1) b)
        ((even? b) (double (fast_mult a (half b))))
        (else (+ (fast_mult a (- b 1)) a))))
```
### Exercise 1.18
> Using the results of Exercise 1.16 and Exercise 1.17, devise a procedure that generates an iterative process for multiplying two integers in terms of adding, doubling, and halving, and uses a logarithmic number of steps.

```scheme
(define (double a) (* a 2))

(define (half a) (/ a 2))

(define (faster_mult a b) (mult_iter a b 0))

(define (mult_iter a b n)
  (cond ((or (= b 0) (= a 0)) 0)
        ((= b 1) (+ a n))
        ((even? b) (mult_iter (double a) (half b) n))
        (else (mult_iter a (- b 1) (+ a n)))))
```
### Exercise 1.19
> There is a clever algorithm for computing the Fibonacci numbers in a logarithmic number of steps. Recall the transformation of the state variables $a$ and $b$ in the `fib-iter` process of Section 1.2.2: $a \leftarrow a+b$ and $b \leftarrow a$. Call this transformation $T$, and observe that applying $T$ over and over again $n$ times, starting with 1 and 0, produces the pair Fib($n+1$) and Fib($n$). In other words, the Fibonacci numbers are produced by applying $T^n$, the $n^{th}$ power of the transformation $T$, starting with the pair (1, 0). Now consider $T$ to be the special case of $p=0$ and $q=1$ in a family of transformations $T_{pq}$, where $T_{pq}$ transforms the pair ($a$, $b$) according to $a \leftarrow bq+aq+ap$ and $b \leftarrow bp + aq$. Show that if we apply such a transformation $T_{pq}$ twice, the effect is the same as using a simple transformation $T_{p'q'}$ of the same form, and compute $p'$ and $q'$ in terms of $p$ and $q$. This gives us an explicit way to square these transformations, and thus we can compute $T^n$ using successive squaring, as in the `fast-expt` procedure. Put this all together to complete the following procedure, which runs in a logarithmic number of steps:
```scheme
(define (fib n)
  (fib-iter 1 0 0 1 n))
(define (fib-iter a b p q count)
  (cond ((= count 0) b)
        ((even? count)
          (fib-iter a
                    b
                    <??>    ; compute p'
                    <??>    ; compute q'
                    (/ count 2)))
        (else (fib-iter (+ (* b q) (* a q) (* a p))
                        (+ (* b p) (* a q))
                        pq
                        (- count 1)))))
```

Using simultaneous equations, we can solve to find out the next two steps of $q$ and $p$:
$a' = bq+aq+ap$
$b' = bp+aq$
using $a=1,b=0$, we can simplify the equation to be:
$b' = q$
$a' = q+p$
or
$q=b'$
$p=a'-q$
where $a=1,b=0,a'=2,b'=1$
$q=1$
$p=1$
I have a sneaking suspicion that we are fibonacci'ing $q$ and $p$.
$a=1,b=0,p=1,q=1$
$a=2,b=1,p=1,q=1$
$a=5,b=3,p=1,q=1$
where $a=1,b=0,a'=5,b'=3$
$q=3$
$p=2$
```scheme
(define (fib n)
  (fib-iter 1 0 0 1 n))
(define (fib-iter a b p q count)
  (cond ((= count 0) b)
        ((even? count)
          (fib-iter a
                    b
                    (+ (* p p) (* q q))
                    (+ (* p q) (* q q) (* q p))
                    (/ count 2)))
        (else (fib-iter (+ (* b q) (* a q) (* a p))
                        (+ (* b p) (* a q))
                        p
                        q
                        (- count 1)))))
```
### 1.2.5: Greatest Common Divisor
Here we consider finding the greatest common divisor. Euclid's Algorithm is introduced (below), and Lame's Theorem is explained in the most confusing possible format, a plain-text footnote.
```scheme
(define (gcd a b)
  (if (= b 0)
      a
      (gcd b (remainder a b))))
```

##### Exercise 1.20
> The process that a procedure generates is of course dependent on the rules used by the interpreter. As an example, consider the iterative `gcd` procedure given above. Suppose we were to interpret this procedure using normal-order evaluation, as discussed in Section 1.1.5. (The normal-order-evaluation rule for `if` is described in Exercise 1.5.) Using the substitution method (for normal order), illustrate the process generated in evaluating `(gcd 206 40)` and indicate the `remainder` operations that are actually performed. How many remainder operations are actually performed in the normal-order evaluation of `(gcd 206 40)`? In the applicative-order evaluation?

For normal-order, the if conditional evaluates first, then we expand again the true/false branch accordingly, without evaluating the arguments.
```scheme
(gcd 206 40)

(if (= 40 0)
  206
  (gcd 40 (remainder 206 40))))

(if (= (remainder 206 40) 0)  ; 1 evaluation
  40
  (gcd (remainder 206 40) (remainder 40 (remainder 206 40)))))

(if (= (remainder 40 (remainder 206 40)) 0)  ; 2 evaluations
  (remainder 206 40)
  (gcd (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))))

(if (= (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) 0)  ; 4 evaluations
  (remainder 40 (remainder 206 40))
  (gcd (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))

if (= (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))) 0)  ; 7 evaluations
  (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))
  (remainder (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40)))) (remainder (remainder (remainder 206 40) (remainder 40 (remainder 206 40))) (remainder (remainder 40 (remainder 206 40)) (remainder (remainder 206 40) (remainder 40 (remainder 206 40))))))

(remainder (remainder 206 40) (remainder 40 (remainder 206 40)))  ; 4 evaluations
```
In total, 18 remainder operations are performed.

For applicative-order evaluation, the arguments are evaluated before passing them into another operation.
```scheme
(gcd 206 40)

(if (= 40 0)
  206
  (gcd 40 (remainder 206 40))))  ; 1 evaluation

(if (= 6 0)
  40
  (gcd 6 (remainder 40 6)))  ; 1 evaluation

(if (= 4 0)
  6
  (gcd 4 (remainder 6 4)))  ; 1 evaluation

(if (= 2 0)
  4
  (gcd 2 (remainder 4 2)))  ; 1 evaluation

(if (= 0 0)
  2
  (gcd 0 (remainder 2 0)))
```
In total, 4 remainder operations are performed.
### 1.2.6: Example: Testing for Primacy
Here we talk about testing to see if a number is prime. One way, taking $Θ(\sqrt{n})$ time (below) is to check all numbers up to $\sqrt{n}$ to see if $n$ mod $x$ is 0.
```scheme
(define (square x) (* x x))

(define (prime? n)
  (= n (smallest-divisor n)))
(define (smallest-divisor n) (find-divisor n 2))
(define (find-divisor n test-divisor)
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (+ test-divisor 1)))))
(define (divides? a b) (= (remainder b a) 0))
```
Another way is to use Fermat's Test, which takes $Θ(log(n))$ time, using Fermat's Little Theorem, which states "If $n$ is a prime number, and $a$ is any positive integer less than $n$, then $a$ raised to the $n^{th}$ power is congruent to $a$ module $n$."
```scheme
(define (fast-prime? n times)
  (cond ((= times 0) true)
        ((fermat-test n) (fast-prime? n (- times 1)))
        (else false)))

(define (fermat-test n)
  (define (try-it a)
    (= (expmod a n n) a))
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder
          (square (expmod base (/ exp 2) m))
          m))
        (else
         (remainder
          (* base (expmod base (- exp 1) m))
          m))))
```
##### Exercise 1.21
> Use the `smallest-divisor` procedure to find the smallest divisor for each of the following numbers: 199, 1999, 19999.

199, 1999, 7
##### Exercise 1.22
> Most Lisp implementations include a primitive called `runtime` that returns an integer that specifies the amount of time the system has been running (measured, for example, in microseconds). The following `timed-prime-test` procedure, when called with an integer $n$, prints $n$ and checks to see if $n$ is prime. If $n$ is prime, the procedure prints three asterisks followed by the amount of time used in performing the test.
```scheme
(define (timed-prime-test n)
  (newline)
  (display n)
  (start-prime-test n (runtime)))
(define (start-prime-test n start-time)
  (if (prime? n)
      (report-prime (- (runtime) start-time))))
(define (report-prime elapsed-time)
  (display " *** ")
  (display elapsed-time))
```
> Using this procedure, write a procedure `search-for-primes` that checks the primality of consecutive odd integers in a specified range. Use your procedure to find the three smallest primes larger than 1000; larger than 10,000; larger than 100,000; larger than 1,000,000. Note the time needed to test each prime. Since the testing algorithm has order of growth of $Θ(\sqrt{n})$, you should expect that testing for primes around 10,000 should take about $\sqrt{10}$ times as long as testing for primes around 1000. Do your timing data bear this out? How well do the data for 100,000 and 1,000,000 support the $Θ(\sqrt{n})$ prediction? Is your result compatible with the notion that programs on your machine run in time proportional to the number of steps required for the computation?

```scheme
(define (search-for-primes start end)
  (timed-prime-test start)
  (if (< start end) (search-for-primes (+ start 1) end)))
```

Ideally, we would instead return true if we find a prime, and limit the number of numbers searched. We would also not print every number.
```scheme
(define (start-prime-test n start-time)
  (if (prime? n)
      (begin
        (newline)
        (display n)
        (report-prime (- (runtime) start-time))
        #t)
      #f))

(define (search-for-primes start num)
  (if (> num 0)
      (search-for-primes (+ start 1) (if (timed-prime-test start) (- num 1) num))
      (begin (newline) (display "Done"))))
```

And given the fickle nature of code runtimes, we can go further to define a benchmarking function
```scheme
(define (timed-prime-test n)
  (benchmark-prime-test n 1000 0 0 (runtime)))

(define (benchmark-prime-test n repeats count total-time start-time)
  (if (prime? n)
      (begin
        (set! total-time (+ total-time (- (runtime) start-time)))
        (if (< count repeats)
            (benchmark-prime-test n  repeats (+ count 1) total-time (runtime))
            (begin
              (newline)
              (display n)
              (report-prime (/ total-time repeats))
              #t))
      )
      #f))

(define (quick-search)
  (search-for-primes 1000 3)
  (search-for-primes 10000 3)
  (search-for-primes 100000 3)
  (search-for-primes 1000000 3))
```

| Prime   | Time                | Decimal |
| ------- | ------------------- | ------- |
| 1009    | $\frac{621}{1000}$  | .621    |
| 1013    | $\frac{741}{1000}$  | .741    |
| 1019    | $\frac{643}{1000}$  | .643    |
| 10007   | $\frac{583}{250}$   | 2.332   |
| 10009   | $\frac{2373}{1000}$ | 2.373   |
| 10037   | $\frac{203}{100}$   | 2.03    |
| 100003  | $\frac{799}{125}$   | 6.392   |
| 100019  | $\frac{803}{125}$   | 6.424   |
| 100043  | $\frac{6061}{1000}$ | 6.061   |
| 1000003 | $\frac{9851}{500}$  | 19.702  |
| 1000033 | $\frac{9349}{500}$  | 18.698  |
| 1000037 | $\frac{9827}{500}$  | 19.654  |

| Primes > | Avg Time | Increase |
| -------- | -------- | -------- |
| 1000     | .6683    | -        |
| 10000    | 2.245    | ~3.36x   |
| 100000   | 6.2923   | ~2.803x  |
| 1000000  | 19.3513  | ~3.075x  |
These numbers are a little scattered, and we can see the average times for 1000 runs are still varying quite widely. Overall, the average growth seems to be ~$\sqrt{10}$. This supports our understanding of how long computers take to run procedures based on the number of steps, though the actual result might vary and be less or more than expected.
##### Exercise 1.23
> The `smallest-divisor` procedure shown at the start of this section does lots of needless testing: After it checks to see if the number is divisible by 2 there is no point in checking to see if it is divisible by any larger even numbers. This suggests that the values used for `test-divisor` should not be 2, 3, 4, 5, 6, ..., but rather 2, 3, 5, 7, 9, .... To implement this change, define a procedure `next` that returns 3 if its input is equal to 2 and otherwise returns its input plus 2. Modify the `smallest-divisor` procedure to use `(next test-divisor)` instead of `(+ test-divisor 1)`. With `timed-prime-test` incorporating this modified version of `smallest-divisor`, run the test for each of the 12 primes found in Exercise 1.22. Since this modification halves the number of test steps, you should expect it to run about twice as fast. Is this expectation confirmed? If not, what is the observed ratio of the speeds of the two algorithms, and how do you explain the fact that it is different from 2?

```scheme
(define (next x) (if (= x 2) 3 (+ x 2)))
(define (find-divisor n test-divisor)
  (cond ((> (square test-divisor) n) n)
        ((divides? test-divisor n) test-divisor)
        (else (find-divisor n (next test-divisor)))))
```

| Prime   | Time                 | Decimal |
| ------- | -------------------- | ------- |
| 1009    | $\frac{421}{1000}$   | .421    |
| 1013    | $\frac{227}{500}$    | .454    |
| 1019    | $\frac{483}{1000}$   | .483    |
| 10007   | $\frac{1149}{1000}$  | 1.149   |
| 10009   | $\frac{643}{500}$    | 1.286   |
| 10037   | $\frac{1399}{1000}$  | 1.399   |
| 100003  | $\frac{499}{125}$    | 3.992   |
| 100019  | $\frac{671}{200}$    | 3.355   |
| 100043  | $\frac{3737}{1000}$  | 3.737   |
| 1000003 | $\frac{10449}{1000}$ | 10.449  |
| 1000033 | $\frac{5127}{500}$   | 10.254  |
| 1000037 | $\frac{10991}{1000}$ | 10.991  |

| Primes > | Avg Time | Increase | Improvement |
| -------- | -------- | -------- | ----------- |
| 1000     | .4527    | -        | 1:~1.48     |
| 10000    | 1.278    | ~2.82x   | 1:~1.76     |
| 100000   | 3.6947   | ~2.89x   | 1:~1.7      |
| 1000000  | 10.5647  | ~2.86x   | 1:~1.83     |
We can see a slower logarithmic growth, from $\sqrt{10}$ down to $\sqrt{8}$, and we can see our time improvements slowly getting closer and closer to the 1:2 ratio that we expect. It is likely that cutting out almost half of all numbers to check doesn't increase performance as much as we would expect because we still have some level of overhead in our work. Our `next` procedure is doing an `if` conditional, and then doing the addition. As such, there is a procedure call, and `if` conditional, and an addition in place of what was just an addition.
##### Exercise 1.24
> Modify the `timed-prime-test` procedure of Exercise 1.22 to use `fast-prime?` (the Fermat method), and test each of the 12 primes you found in that exercise. Since the Fermat test has $Θ(log(n))$ growth, how would you expect the time to test primes near 1,000,000 to compare with the time needed to test primes near 1000? Do your data bear this out? Can you explain any discrepancy you find?

We would expect the time needed to test primes at 1,000,000 to be roughly twice as much as 1000, as $log(1000000) = 2*log(1000)$.

For the following test, we are using `(fast-prime? n 4)`

| Prime   | Time                | Decimal |
| ------- | ------------------- | ------- |
| 1009    | $\frac{293}{125}$   | 2.344   |
| 1013    | $\frac{513}{250}$   | 2.052   |
| 1019    | $\frac{2953}{1000}$ | 2.953   |
| 10007   | $\frac{1727}{500}$  | 3.454   |
| 10009   | $\frac{663}{250}$   | 2.652   |
| 10037   | $\frac{131}{50}$    | 2.62    |
| 100003  | $\frac{337}{100}$   | 3.37    |
| 100019  | $\frac{173}{50}$    | 3.46    |
| 100043  | $\frac{3833}{1000}$ | 3.833   |
| 1000003 | $\frac{1669}{500}$  | 3.338   |
| 1000033 | $\frac{379}{100}$   | 3.79    |
| 1000037 | $\frac{709}{200}$   | 3.545   |
We can see that our expectation doesn't quite pan out. Our primes around 1000 took on average 2.45, while our primes around 1,000,000 took 3.558, or ~1.45x the time of those at 1000. Though this rise IS logarithmic, it is likely that using more or less random numbers would alter the growth.
##### Exercise 1.25
> Alyssa P. Hacker complains that we went to a lot of extra work in writing $expmod$. After all, she says, since we already know how to compute exponentials, we could have simply written
```scheme
(define (expmod base exp m)
  (remainder (fast-expt base exp) m))
```
> Is she correct? Would this procedure serve as well for our fast prime tester? Explain.

This would not be the same, as `expmod` takes the remainder at every step of the process. This solution only takes a remainder right at the end, which means we end up with a colossal number. Let's look at our first prime, 1009, and take a random number, for ease let's use, 100. Using Alyssa's version, we end up with $100^{1009}$, which would being $1*10^{2018}$, which takes up about $6704$ bits. This is ${838}$ bytes of data, close to a full MB. If we using our primes greater than 1,000,000, we are going to be taking up full GB of data for this one number. This is wildly infeasible.
##### Exercise 1.26
> Louis Reasoner is having great difficulty doing Exercise 1.24. His `fast-prime?` test seems to run more slowly than his `prime?` test. Louis calls his friend Eva Lu Ator over to help. When they examine Louis’s code, they find that he has rewritten the `expmod` procedure to use an explicit multiplication, rather than calling `square`:
```scheme
(define (expmod base exp m)
  (cond ((= exp 0) 1)
		((even? exp)
		 (remainder (* (expmod base (/ exp 2) m)
					   (expmod base (/ exp 2) m))
					m))
		(else
		 (remainder (* base
					   (expmod base (- exp 1) m))
					m))))
```
> “I don’t see what difference that could make,” says Louis. “I do.” says Eva. “By writing the procedure like that, you have transformed the $Θ(log(n))$ process into a $Θ(n)$ process.” Explain.

By using explicit multiplication, remembering our applicative-order evaluation, we are calling the `expmod` procedure twice instead of once. Given this is a recursive call, we are causing tree recursion, which generally has a performance of $2^n$. This means we are taking $Θ(log(n))$, making it $Θ(log(n^2))$. Remembering our log rules, $log(2^n)=n*log(2) \approx n$, so we end up with $Θ(n)$.
##### Exercise 1.27
> Demonstrate that the Carmichael numbers listed in Footnote 1.47 really do fool the Fermat test. That is, write a procedure that takes an integer $n$ and tests whether $a^n$ is congruent to $a$ modulo $n$ for every $a < n$, and try your procedure on the given Carmichael numbers.

```scheme
(define (fermat-full-test n)
  (display n)
  (if (fermat-test-all-a 1 n)
      (display " is prime")
      (display " is not prime")))

(define (fermat-test-all-a a n)
  (if (< a n)
      (if (= (expmod a n n) a)
          (fermat-test-all-a (+ a 1) n)
          #f)
      #t))
```

All are considered prime.
```scheme
> (fermat-full-test 2)
2 is prime
> (fermat-full-test 3)
3 is prime
> (fermat-full-test 4)
4 is not prime
> (fermat-full-test 1009)
1009 is prime
> (fermat-full-test 561)
561 is prime
> (fermat-full-test 1105)
1105 is prime
> (fermat-full-test 1729)
1729 is prime
> (fermat-full-test 2465)
2465 is prime
> (fermat-full-test 2821)
2821 is prime
> (fermat-full-test 6601)
6601 is prime
```
##### Exercise 1.28
> One variant of the Fermat test that cannot be fooled is called the Miller-Rabin test (Miller 1976; Rabin 1980). This starts from an alternate form of Fermat’s Little Theorem, which states that if $n$ is a prime number and $a$ is any positive integer less than $n$, then $a$ raised to the $(n-1)$-st power is congruent to 1 modulo $n$. To test the primality of a number $n$ by the Miller-Rabin test, we pick a random number $a < n$ and raise $a$ to the $(n-1)$-st power modulo $n$ using the `expmod` procedure. However, whenever we perform the squaring step in `expmod`, we check to see if we have discovered a “nontrivial square root of 1 modulo $n$,” that is, a number not equal to 1 or $n-1$ whose square is equal to 1 modulo $n$. It is possible to prove that if such a nontrivial square root of 1 exists, then $n$ is not prime. It is also possible to prove that if $n$ is an odd number that is not prime, then, for at least half the numbers $a < n$, computing $a^{n-1}$ in this way will reveal a nontrivial square root of 1 modulo $n$. (This is why the Miller-Rabin test cannot be fooled.) Modify the `expmod` procedure to signal if it discovers a nontrivial square root of 1, and use this to implement the Miller-Rabin test with a procedure analogous to `fermat-test`. Check your procedure by testing various known primes and non-primes. Hint: One convenient way to make `expmod` signal is to have it return 0.

```scheme
(define (miller-rabin-test n)
  (define (try-it a)
    (= (expmod a (- n 1) n) 1))
  (try-it (+ 1 (random (- n 1)))))

(define (expmod base exp m)
  (cond ((= exp 0) 1)
        ((even? exp)
         (remainder-square (expmod base (/ exp 2) m) m))
        (else
         (remainder
          (* base (expmod base (- exp 1) m))
          m))))

(define (remainder-square x n)
  (define remainder-squared (remainder (square x) n))
  (if (and (= remainder-squared 1)
           (not (or (= x 1)
                    (= x (- n 1)))))
      0
      remainder-squared))

(define (miller-rabin-full-test n)
  (newline)
  (display n)
  (if (miller-rabin-test-all-a 1 n)
      (display " is prime")
      (display " is not prime")))

(define (miller-rabin-test-all-a a n)
  (if (< a n)
      (if (= (expmod a (- n 1) n) 1)
          (miller-rabin-test-all-a (+ a 1) n)
          #f)
      #t))
```

A full nightmare, took me a while to realise that `expmod` was supposed to equal 1.